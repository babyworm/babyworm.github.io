<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robots.txt on Babyworm Hugo Site</title>
    <link>http://localhost:8080/tags/robots.txt/</link>
    <description>Recent content in Robots.txt on Babyworm Hugo Site</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>babyworm@gmail.com (babyworm)</managingEditor>
    <webMaster>babyworm@gmail.com (babyworm)</webMaster>
    <copyright>© Babyworm, All Rights Reserved.</copyright>
    <lastBuildDate>Tue, 14 Nov 2006 10:45:29 +0000</lastBuildDate>
    <atom:link href="http://localhost:8080/tags/robots.txt/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>결국 다음 봇 차단 결정</title>
      <link>http://localhost:8080/archives/195/</link>
      <pubDate>Tue, 14 Nov 2006 10:45:29 +0000</pubDate><author>babyworm@gmail.com (babyworm)</author>
      <guid>http://localhost:8080/archives/195/</guid>
      <description>트래픽을 몰리는 원인 분석 결과.. 가장 큰 원인이 여러개의 다음 봇이 동시에 접근하고 있는 것을 발견했습니다. 제가 알기로 다음봇은 구글봇과 동일해서 robots.txt 규칙을 잘 지키는 것으로 알고 있었는데.. 어쩐 일인지, 여러개의 로봇이 돌아가면서 접근해서 엄청나게 트래픽을 잡아먹네요.. robots.txt 규칙을 따른다면 여러 봇이 오더라도 3시간에 한번씩만 가져가야 하니</description>
    </item>
    <item>
      <title>트래픽 초과… 로봇의 힘..</title>
      <link>http://localhost:8080/archives/131/</link>
      <pubDate>Fri, 20 Oct 2006 00:40:34 +0000</pubDate><author>babyworm@gmail.com (babyworm)</author>
      <guid>http://localhost:8080/archives/131/</guid>
      <description>어제 오후에 갑작스럽게 트래픽이 초과되는 일이 발생했습니다. 제 홈페이지와 보드, Wikipage는 그야말로 개인적으로 사용하니 별문제 없겠고.. 블로그도 그리 인기있을만한 블로그가 아니니 방문자로 인한것이 아닌듯 하더군요.. (블로그 상에는 많은 사용자 방문수가 나오는데.. Google analysis 결과로 미루어보아 실제 방문자수는 하루에 50명 이하일</description>
    </item>
  </channel>
</rss>
